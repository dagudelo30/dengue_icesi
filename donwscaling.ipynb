{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDVI a 0.05 grados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Especifica el patrón para encontrar los archivos .tif\n",
    "ruta = \"E:/Climaticas/ICESI/ICESI/output/data/climate/raw_data/ndvi/*.tif\"\n",
    "tif_files = glob.glob(ruta)\n",
    "print(\"Archivos encontrados:\", tif_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_arrays = []\n",
    "\n",
    "for file in tif_files:\n",
    "    # Extraer el nombre base del archivo\n",
    "    basename = os.path.basename(file)\n",
    "    # La estructura del nombre es: \n",
    "    # AVHRR-Land_v005_AVH13C1_NOAA-19_20130101_c20170408020753\n",
    "    # Suponiendo que la fecha se encuentra en la quinta parte (índice 4)\n",
    "    partes = basename.split('_')\n",
    "    date_str = partes[4]  # '20130101'\n",
    "    # Convertir a objeto datetime\n",
    "    fecha = pd.to_datetime(date_str, format=\"%Y%m%d\")\n",
    "    \n",
    "    # Abrir el archivo .tif con rioxarray\n",
    "    da = rxr.open_rasterio(file)\n",
    "    # Si da tiene dimensiones (band, y, x), se añade una dimensión 'time'\n",
    "    da = da.expand_dims(time=[fecha])\n",
    "    \n",
    "    data_arrays.append(da)\n",
    "\n",
    "# Concatenar todos los DataArrays a lo largo de la dimensión 'time'\n",
    "ds_combined = xr.concat(data_arrays, dim=\"time\")\n",
    "print(ds_combined)\n",
    " \n",
    "\n",
    "ds_combined = ds_combined.where(ds_combined != -9999, np.nan)\n",
    "\n",
    "ds_combined = ds_combined.squeeze('band', drop=True)\n",
    "ds_combined[0,:,:].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_combined[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperatura maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el patrón para encontrar los archivos .nc\n",
    "ruta = \"E:/Climaticas/ICESI/ICESI/output/data/climate/raw_data/temperature_max/*.nc\"\n",
    "\n",
    "nc_files = glob.glob(ruta)\n",
    "print(\"Cantidad de archivos encontrados:\", len(nc_files))\n",
    "\n",
    "\n",
    "ds_list = []\n",
    "\n",
    "for file in nc_files:\n",
    "    ds = xr.open_dataset(file, chunks={})\n",
    "    # Verifica que el dataset tenga la coordenada \"time\"\n",
    "    if \"time\" not in ds.coords:\n",
    "        raise ValueError(f\"El archivo {file} no tiene la coordenada 'time'\")\n",
    "    ds_list.append(ds)\n",
    "\n",
    "\n",
    "# Concatenar todos los datasets a lo largo de la dimensión \"time\"\n",
    "ds_temp_max = xr.concat(ds_list, dim=\"time\")\n",
    "ds_temp_max =ds_temp_max.Temperature_Air_2m_Max_24h\n",
    "print(ds_temp_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match por date y resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dates = np.intersect1d(ds_combined.time, ds_temp_max.time)\n",
    "\n",
    "# Seleccionamos solo las fechas comunes en cada dataset\n",
    "ds_high_common = ds_combined.sel(time=common_dates)\n",
    "ds_coarse_common = ds_temp_max.sel(time=common_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_high_common = ds_high_common.rename({\"x\": \"lon\", \"y\": \"lat\"})\n",
    "ds_high_regridded = ds_high_common.interp(\n",
    "    lon=ds_coarse_common.lon, \n",
    "    lat=ds_coarse_common.lat,\n",
    "    method=\"linear\"\n",
    ")\n",
    "\n",
    "\n",
    "# Ahora ds_high_regridded y ds_coarse_common tienen:\n",
    "# - Las mismas fechas (dimensión \"time\")\n",
    "# - La misma malla espacial (coordenadas \"lat\" y \"lon\")\n",
    "\n",
    "print(ds_high_regridded)\n",
    "print(ds_coarse_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_combined[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_high_regridded[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_coarse_common[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregación por semana epidemiologica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def start_of_epi_year(Y: int) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    Devuelve el domingo que define el inicio de la semana 1 del año Y,\n",
    "    según la regla:\n",
    "      - La 'semana 1' comienza en el primer domingo entre\n",
    "        29 de diciembre de Y-1 y 4 de enero de Y.\n",
    "    \"\"\"\n",
    "    dec29 = pd.Timestamp(Y - 1, 12, 29)\n",
    "    jan4  = pd.Timestamp(Y, 1, 4)\n",
    "    # Día de la semana (lunes=0 ... domingo=6)\n",
    "    w = dec29.weekday()\n",
    "    # Offset para encontrar el siguiente domingo a partir de dec29\n",
    "    offset = (6 - w) % 7\n",
    "    first_sunday = dec29 + pd.Timedelta(days=offset)\n",
    "\n",
    "    # Si por algún motivo ese domingo rebasara el 4 de enero de Y, retrocedemos 7 días\n",
    "    if first_sunday > jan4:\n",
    "        first_sunday -= pd.Timedelta(days=7)\n",
    "\n",
    "    return first_sunday\n",
    "\n",
    "\n",
    "def get_epi_year_week(date: pd.Timestamp) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Dada una fecha (Timestamp), retorna (epi_year, epi_week)\n",
    "    según la convención:\n",
    "      - Semanas van de domingo a sábado.\n",
    "      - La semana 1 de un año Y empieza en start_of_epi_year(Y).\n",
    "      - Puede existir semana 53 si sobran >=4 días al final del año.\n",
    "    \"\"\"\n",
    "    # (1) Encontrar el domingo de la semana que contiene 'date'.\n",
    "    #     weekday(): lunes=0, ..., domingo=6\n",
    "    offset = (date.weekday() + 1) % 7  # 0 si es domingo, 1 si es lunes...\n",
    "    sunday_of_date = date - pd.Timedelta(days=offset)\n",
    "\n",
    "    # (2) Decidir si este domingo corresponde al año sunday_of_date.year o al siguiente\n",
    "    #     - Comparamos con start_of_epi_year(...) del \"año + 1\"\n",
    "    candidate_next_year_start = start_of_epi_year(sunday_of_date.year + 1)\n",
    "\n",
    "    if sunday_of_date < candidate_next_year_start:\n",
    "        # Está dentro del año epidemiológico actual\n",
    "        epi_year = sunday_of_date.year\n",
    "    else:\n",
    "        # Pertenece al siguiente\n",
    "        epi_year = sunday_of_date.year + 1\n",
    "\n",
    "    # (3) Calcular cuántas semanas han pasado desde el inicio de epi_year\n",
    "    start_of_year = start_of_epi_year(epi_year)\n",
    "    delta_days = (sunday_of_date - start_of_year).days\n",
    "    epi_week = (delta_days // 7) + 1  # semana 1, 2, 3, etc.\n",
    "\n",
    "    return epi_year, epi_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la semana epidemiológica para cada fecha y agregarla como coordenada\n",
    "epi_weeks = []\n",
    "for t in ds_high_regridded.time.values:\n",
    "    t_pd = pd.Timestamp(t)\n",
    "    epi_year, epi_week = get_epi_year_week(t_pd)\n",
    "    # Formateamos como \"AÑO-SEMA\" (ej.: \"2013-01\")\n",
    "    epi_weeks.append(f\"{epi_year}-{epi_week:02d}\")\n",
    "\n",
    "da = ds_high_regridded.assign_coords(epi_week=(\"time\", epi_weeks))\n",
    "\n",
    "# Agrupar por semana epidemiológica y calcular el máximo para cada grupo\n",
    "da_NDVI_max = da.groupby(\"epi_week\").max(\"time\", skipna=True)\n",
    "\n",
    "print(da_NDVI_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la semana epidemiológica para cada fecha y agregarla como coordenada\n",
    "epi_weeks = []\n",
    "for t in ds_coarse_common.time.values:\n",
    "    t_pd = pd.Timestamp(t)\n",
    "    epi_year, epi_week = get_epi_year_week(t_pd)\n",
    "    # Formateamos como \"AÑO-SEMA\" (ej.: \"2013-01\")\n",
    "    epi_weeks.append(f\"{epi_year}-{epi_week:02d}\")\n",
    "\n",
    "da = ds_coarse_common.assign_coords(epi_week=(\"time\", epi_weeks))\n",
    "\n",
    "# Agrupar por semana epidemiológica y calcular el máximo para cada grupo\n",
    "da_tmax_mean = da.groupby(\"epi_week\").mean(\"time\", skipna=True)\n",
    "\n",
    "print(da_tmax_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_pixel = da_tmax_mean.stack(pixel=(\"lat\", \"lon\"))\n",
    "da_transposed = da_pixel.transpose(\"pixel\", \"epi_week\")\n",
    "y = da_transposed.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_pixel_NDVI = da_NDVI_max.stack(pixel=(\"lat\", \"lon\"))\n",
    "da_NDVI_transposed = da_pixel_NDVI.transpose(\"pixel\", \"epi_week\")\n",
    "X = da_NDVI_transposed.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelación con xgbost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en conjunto de entrenamiento y validación\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Asegurarse de que X_train y X_valid sean 2D\n",
    "if X_train.ndim == 1:\n",
    "    X_train = X_train.reshape(-1, 1)\n",
    "if X_valid.ndim == 1:\n",
    "    X_valid = X_valid.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para X_train y y_train:\n",
    "mask_train = ~np.isnan(y_train) & ~np.isnan(X_train).any(axis=1)\n",
    "X_train = X_train[mask_train]\n",
    "y_train = y_train[mask_train]\n",
    "\n",
    "# Para X_valid y y_valid:\n",
    "mask_valid = ~np.isnan(y_valid) & ~np.isnan(X_valid).any(axis=1)\n",
    "X_valid = X_valid[mask_valid]\n",
    "y_valid = y_valid[mask_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Asegúrate de que tus datos tengan la forma adecuada:\n",
    "if X_train.ndim == 1:\n",
    "    X_train = X_train.reshape(-1, 1)\n",
    "if X_valid.ndim == 1:\n",
    "    X_valid = X_valid.reshape(-1, 1)\n",
    "\n",
    "def xgb_cv(max_depth, learning_rate, min_child_weight, subsample, colsample_bytree):\n",
    "    # Configurar parámetros\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'seed': 42,\n",
    "    }\n",
    "    \n",
    "    # Preparar los DMatrix para XGBoost\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    model = xgb.train(params, dtrain, num_boost_round=100, verbose_eval=False)\n",
    "    \n",
    "    # Predecir y calcular el RMSE\n",
    "    y_pred = model.predict(dvalid)\n",
    "    rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
    "    \n",
    "    # Retornamos el negativo de RMSE (para maximizarlo)\n",
    "    return -rmse\n",
    "\n",
    "# Definir el espacio de hiperparámetros a explorar\n",
    "pbounds = {\n",
    "    'max_depth': (3, 10),\n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'min_child_weight': (1, 10),\n",
    "    'subsample': (0.5, 1.0),\n",
    "    'colsample_bytree': (0.5, 1.0)\n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=xgb_cv,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=3, n_iter=50)\n",
    "\n",
    "# Imprimir el mejor resultado encontrado\n",
    "print(\"Mejor resultado:\", optimizer.max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Concatenar los datos de entrenamiento y validación\n",
    "X_full = np.vstack([X_train, X_valid])\n",
    "y_full = np.concatenate([y_train, y_valid])\n",
    "\n",
    "# 2. Crear el DMatrix para todos los datos\n",
    "dfull = xgb.DMatrix(X_full, label=y_full)\n",
    "\n",
    "# 3. Definir los parámetros del modelo usando los hiperparámetros óptimos obtenidos\n",
    "# Suponiendo que optimizer.max['params'] contiene los mejores hiperparámetros:\n",
    "params_full = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': int(optimizer.max['params']['max_depth']),\n",
    "    'learning_rate': optimizer.max['params']['learning_rate'],\n",
    "    'min_child_weight': optimizer.max['params']['min_child_weight'],\n",
    "    'subsample': optimizer.max['params']['subsample'],\n",
    "    'colsample_bytree': optimizer.max['params']['colsample_bytree'],\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "# 4. Entrenar el modelo en el conjunto completo\n",
    "model_full = xgb.train(params_full, dfull, num_boost_round=100, verbose_eval=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Extraer la capa en tiempo deseada (por ejemplo, la posición 0)\n",
    "time_index = 0\n",
    "data_slice = ds_combined.isel(time=time_index)\n",
    "\n",
    "# 2. Aplanar la capa para formar una matriz (n_samples, n_features)\n",
    "# Aquí n_features es 1 (una sola variable predictora)\n",
    "X_new = data_slice.values.flatten().reshape(-1, 1)\n",
    "\n",
    "# 3. Preparar la matriz para XGBoost\n",
    "dnew = xgb.DMatrix(X_new)\n",
    "\n",
    "# 4. Predecir con el modelo\n",
    "y_pred_new = model_full.predict(dnew)\n",
    "\n",
    "# 5. Reestructurar la predicción a la forma espacial original\n",
    "prediction_map = y_pred_new.reshape(data_slice.shape)\n",
    "\n",
    "# 6. Graficar el resultado\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(prediction_map, cmap='viridis')\n",
    "plt.title(\"Predicción para la posición en tiempo {}\".format(time_index))\n",
    "plt.colorbar(label=\"Valor predicho\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.imshow(da_tmax_mean[0, :, :], cmap='viridis', vmin=283, vmax=301)\n",
    "plt.title(\"Temperatura AGERA5\")\n",
    "plt.colorbar(label=\"Valor predicho\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import Rbf\n",
    "\n",
    "# Supongamos que 'prediction_map' es el resultado de la predicción con forma (ny, nx)\n",
    "ny, nx = prediction_map.shape\n",
    "\n",
    "# Crear las coordenadas de la malla (por ejemplo, índices de filas y columnas)\n",
    "x = np.arange(nx)\n",
    "y = np.arange(ny)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Aplanar las coordenadas y el mapa de predicción\n",
    "X_flat = X.flatten()\n",
    "Y_flat = Y.flatten()\n",
    "Z_flat = prediction_map.flatten()\n",
    "\n",
    "# Configurar la interpolación RBF\n",
    "# Puedes probar diferentes funciones ('multiquadric', 'inverse', 'gaussian', etc.) y parámetros de suavizado\n",
    "rbf_interpolator = Rbf(X_flat, Y_flat, Z_flat, function='multiquadric', smooth=0)\n",
    "\n",
    "# Evaluar el interpolador en la malla original (o en una malla más densa, si lo prefieres)\n",
    "Z_rbf = rbf_interpolator(X, Y)\n",
    "\n",
    "# Graficar el resultado\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(Z_rbf, cmap='viridis',vmin=283, vmax=301)\n",
    "plt.title(\"Temperatura Downscaling\")\n",
    "plt.colorbar(label=\"Valor predicho\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Asegúrate de que tus datos tengan la forma adecuada:\n",
    "if X_train.ndim == 1:\n",
    "    X_train = X_train.reshape(-1, 1)\n",
    "if X_valid.ndim == 1:\n",
    "    X_valid = X_valid.reshape(-1, 1)\n",
    "\n",
    "def xgb_cv(max_depth, learning_rate, min_child_weight, subsample, colsample_bytree):\n",
    "    # Configurar parámetros\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'seed': 42,\n",
    "    }\n",
    "    \n",
    "    # Preparar los DMatrix para XGBoost\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    model = xgb.train(params, dtrain, num_boost_round=100, verbose_eval=False)\n",
    "    \n",
    "    # Predecir y calcular el RMSE\n",
    "    y_pred = model.predict(dvalid)\n",
    "    rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
    "    \n",
    "    # Retornamos el negativo de RMSE (para maximizarlo)\n",
    "    return -rmse\n",
    "\n",
    "# Definir el espacio de hiperparámetros a explorar\n",
    "pbounds = {\n",
    "    'max_depth': (3, 10),\n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'min_child_weight': (1, 10),\n",
    "    'subsample': (0.5, 1.0),\n",
    "    'colsample_bytree': (0.5, 1.0)\n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=xgb_cv,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=3, n_iter=50)\n",
    "\n",
    "# Imprimir el mejor resultado encontrado\n",
    "print(\"Mejor resultado:\", optimizer.max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob\n",
    "\n",
    "# Especifica el patrón que abarque tus 5 archivos .nc \n",
    "# (modifica la ruta y el patrón si es necesario)\n",
    "\n",
    "import glob\n",
    "\n",
    "path = \"E:/data_climatica/relative_humidity/*.nc\"\n",
    "archivos = glob.glob(path)\n",
    "print(\"Archivos encontrados:\", archivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(archivos[0], engine='scipy')\n",
    "print(ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
